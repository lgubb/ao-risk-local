version: "3.9"

services:
  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    container_name: ao-risk-api
    restart: unless-stopped
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/code
      # Backend secrets and config (read from host env or .env)
      - SECRET_KEY=${SECRET_KEY}
      - AUTH_REQUIRED=${AUTH_REQUIRED:-true}
      - ACCESS_TOKEN_EXPIRE_MINUTES=${ACCESS_TOKEN_EXPIRE_MINUTES:-30}
      # LLM config
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-qwen2.5:1.5b-instruct}
      - OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE:-5m}
      - OLLAMA_TEMPERATURE=${OLLAMA_TEMPERATURE:-0.1}
    depends_on:
      - ollama
    volumes:
      - hf-cache:/root/.cache/huggingface
      - ./chroma_db:/code/chroma_db
      - ./storage:/code/storage

  frontend:
    build:
      context: .
      dockerfile: Dockerfile.front
      args:
        # Build-time value used by Next.js to inline the API base URL
        NEXT_PUBLIC_API_BASE: ${NEXT_PUBLIC_API_BASE:-/api}
    container_name: ao-risk-front
    restart: unless-stopped
    environment:
      - NODE_ENV=production
      # Also provide at runtime (useful for SSR/server-only code)
      - NEXT_PUBLIC_API_BASE=${NEXT_PUBLIC_API_BASE:-/api}
    depends_on:
      - api

  nginx:
    image: nginx:alpine
    container_name: ao-risk-nginx
    restart: unless-stopped
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - frontend
      - api

  ollama:
    image: ollama/ollama:latest
    container_name: ao-risk-ollama
    restart: unless-stopped
    volumes:
      - ollama:/root/.ollama

volumes:
  hf-cache:
  ollama:
